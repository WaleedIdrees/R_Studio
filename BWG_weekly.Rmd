---
title: "Cash Deman Forecast App"
output: github_document
---

### Load Libraries 

```{r, setup, echo = FALSE, warning = FALSE, message = FALSE }
rm(list = ls())
knitr::opts_chunk$set(comment = "#>", echo = FALSE, warning = FALSE, message = FALSE, fig.width = 16, fig.height = 9  )


library(odbc)
library(pool)
library(tidyverse)
library(lubridate)
library(fpp3)
library(fable)
library(feasts)
library(tsibble)
library(tsibbledata)
library(prophet)
library(fable.prophet)
library(timeDate)
library(stats)
library(anomaly)
library(anomalize)
library(readxl)
library(furrr)
plan(multisession, workers = 8)


```



## Intoduction: 

link to the r code is here: https://github.com/WaleedIdrees/r_studio/blob/main/BWG_weekly.Rmd

Steps to forecast Cash demand:

* Load Required Libraries: Load the required libraries for the script, including libraries for data manipulation, time series analysis, and plotting.
* Load Data: Load the data for the cash demand for ATM machines and create a summary of the data to check for missing values.
* Preprocessing: Convert the data frame to a tsibble format and check if the time series is complete by using the fill_gaps function. Also, fill any missing values in the    time series data using the interpolate function.
* Visualize the Data: Plot the cash demand over the period of 2016 to 2019 to get an idea of the trend.
* Seasonality Check: Use time series decomposition to check for seasonality in the cash demand data.
* Model Fitting: Fit a time series model, such as ARIMA, to the data and use it to forecast the future cash demand for each ATM machine.

In this note book we Forecast cash Demand for ATM machines. We have the data for ATM machines from 01-06-2016 to 31-12-2019. the cash deman column is represented by withdrawals. We see the summary of the data below we have 24 missing values in withdrawals Column. 

```{r}
## Load Data
df<- readRDS("cash_demand.rds") 
summary(df)
```

```{r}
df$TransDate <- 
  as.Date(df$TransDate)
summary(is.na(df))

```
## Preprocessing

To forcast in r we need to transform our data frame to tsibble format which id time series data format in r for formcasting. We use TransDate column and date index and the key column as the Groups or categories that we want to forecast. 

```{r}
df<-
df %>% 
    as.data.frame() %>%
  group_by(TransDate) %>%
  summarise(withdrawals = sum(withdrawals)) %>%
  tsibble(index = TransDate)


frcst_tsbl<- 
  df %>% 
  dplyr::select( 
    TransDate, withdrawals
    ) %>% 
  as_tsibble(index=TransDate)

```



Check if the time series is complete and fill the missing dates in the dataframe. Fill_gaps feature makes it easy to find out the if there is any missing date index and it fill those gaps with NA values.

```{r}
#If any data for specific dates is missing, it will produce a date for it and then produce an NA value for that date.

frcst_fill_gaps<-
  fill_gaps(frcst_tsbl, .full=FALSE)

frcst_fill_gaps %>% summary()

```
Next, we fill any gaps in the time series data using the fill_gaps function from the tsibble package. The code also adds year, month, and day columns to the tsibble. The missing values in the data are then replaced with regression (TSLM) values using the interpolate function.

```{r}
## we need to take care of NA values in data using interpolate function in R.
## instead of mean or median we use TSLM (regression) values to replace NA values.
final_data <- 
  frcst_fill_gaps %>% 
  model(lm= TSLM(withdrawals~ trend())) %>%
  #model(arima= ARIMA(withdrawals, stepwise = FALSE)) %>% 
  interpolate(frcst_fill_gaps)

summary(final_data)
```

## Graph showing the cash demand over the period of 2016 to 2010

The series look good for forecasting and at first glance it looks stationary. But to confirm we will use ADF and KPSS test

```{r}
final_data %>% autoplot()
```
## Seasonality check 

The graph below shows the time series decomposition of cash demand. The basic idea behind seasonal decomposition is to separate the time series into its underlying components by repeatedly applying a filter to the data. One common method of seasonal decomposition is to use the "additive" method, which assumes that the time series can be modeled as the sum of three components.

```{r}
final_data %>%  
  model(
    STL(log(withdrawals) ~ 
         season(period = "week"),
        #+season(period = "month")+
        #   season(period = "year"),
        robust = TRUE)
  )  %>%
  components() %>%
  autoplot() + labs(x = "Observation")+
  theme(legend.position = "none")
```
From the seasonality plot we see there is no evident trend in the series however we can see a solid weekly seasonality pattern.

## Stationarity Tests

Lets perform ADF test to check for stationarity
Adf test 
Null hypothesis: Series is not stationary, 
Alternate hypothesis = Series is stationary
Kpss test 
Null hypothesis: Series is stationary, 
Alternate hypothesis = Series is not stationary

```{r}
library(tseries)
adf.test(final_data$withdrawals); kpss.test(final_data$withdrawals, null="Trend")

```

Adf test test is significant at 0.05% significance level and we can reject null hypothesis that series is not stationary and accep that series is stationary.
Kpss test is also significant which says that the we reject null hypothesis that series is stationary and accept that series is non stationary.
Null hypothesis for both of these tests are opp

lets repeat both these tests for stationarity after taking first difference .

```{r}
df_diff<- final_data %>% mutate(diff_with= difference(withdrawals, 1 )) %>% drop_na()
adf.test(df_diff$diff_with); kpss.test(df_diff$diff_with, null="Trend")
```

Now both of our tests show that differenced series is stationary. So this tells us that our series is stationary at first difference.
Lets look at acf and pcg graphs to find out whether our series following an ma or ar process or is it more complex.

```{r}
df_diff  %>% autoplot(diff_with)
```

## ACF PCF plots

Now we will identify if our series follows an ar(p) or ma(q) or arma(p,q) process.
figure above shows steps of identifying whethere a series follows an ar(p) or ma(q) or arma(p,q) process.

```{r}
pacman::p_load(png)

img <- readPNG('arma.png')
## Set the plot window dimensions to 8x6 inches
## Set the plot window dimensions to 8x6 inches
par(pty="s", mar=c(0,0,0,0), mai=c(0,0,0,0), xpd=NA)
par(fig=c(0,1,0,1), new=TRUE)
plot(0:1, 0:1, type="n", xlab="", ylab="", axes=FALSE)
rasterImage(img, 0, 0, 1, 1)
```

ACF function finds autocorrelation of previous values to current (lagged values).  Plot of the ACF of shows how autocorrelation coefficients slowly decrease. We do not see a sudden decay in ACF plot so our series is not an MA(q) process but we do detect repeated constant significance lags at lag7 which indicates that our series has weekly seasonality. the process to identifying that the seies follows which process is given in the plot above.
We don't see a sudden decay in PACF plot either. as we can see significant correlations at lags, 1 to 6.
Our series is an ARMA series. 

```{r}
final_data%>%
  gg_tsdisplay(difference(withdrawals,  1),
               plot_type='partial') +
  labs(title="Seasonally differenced")

```

We will use auto_arima function from the forecast package to identify the best ARMA process and SARIMA process for our series. 

```{r}
## fit an ARIMA model using auto.arima()
fit <-  final_data %>% model(sarima= ARIMA( 
  log(withdrawals)~ 0 + pdq( p= 0:5, d=0:2, q= 0:5) + PDQ(P= 0:5,D= 0:2, Q = 0:5), stepwise = FALSE) )
## print the best ARIMA model
print(fit)
```
Auto ARIMA has picked the model with ARIMA(1,0,1)(4,1,0) with weekly seasonality.  

A moving average process, or the moving average (MA) model, states that the current value is linearly dependent on the current and past error terms. The error terms are assumed to be mutually independent and normally distributed, just like white noise. A moving average model is denoted as MA(q), where q is the order. The model expresses the present value as a linear combination of the mean of the series μ, the present error term εt, and past error terms εt–q. The magnitude of the impact of past errors on the present value is quantified using a coefficient denoted as θq. Mathematically, we express a general moving average process of order q as in equation.
The equation for an MA(q) model is defined as:
Y(t) = μ + ε(t) + θ1ε(t-1) + θ2ε(t-2) + ... + θqε(t-q)
where:
Y(t) is the forecast for time period t
μ is the mean of the time series
ε(t) is the error term at time period t
θ1, θ2, ..., θq are the parameters of the model, also known as the MA coefficients, which are estimated from the data. 

The two graph below are showing trend in cash demand over 3 years and demand looks very steady. 

```{r}
final_data %>% 
as.data.frame() %>% 
  mutate(
    year = lubridate::year(TransDate),
    month= lubridate::month(TransDate),
         ) %>% 
  ggplot()+
  aes(x= TransDate, y=withdrawals, col= factor(month) )+
  geom_line()+
  facet_wrap (~ year, scales = "free")


```

## Test & Train Data
```{r}
final_data<-
final_data %>% 
  mutate(
        Days =day(TransDate),
        Weekday_names = weekdays(TransDate),
        Months =month(TransDate)
     )

data_split<- "2019-11-01"
train_data<-
final_data %>% 
  filter(TransDate < data_split) 

forecast_end_date= max( final_data$TransDate)
frct_days= as.double(as.Date(forecast_end_date)+ days(1) -as.Date(data_split) )

future_xreg <-
  new_data(train_data, n = frct_days) %>%
  mutate(
        Days =day(TransDate),
        Weekday_names = weekdays(TransDate),
        Months =month(TransDate)
     )
frct_days
```

We split the data into train and test set . The train set is the data below 2019-11-01 and we will forecast for 61 days.


## Trainig Model  

lets train a seasonal arima model, we can specify as follows and we can tell the function to pick the best model from range of parameters
we will specify an ARIMA mdoel as follows:
pdq(p = 0:5, d = 0:2, q = 0:5)  PDQ(P = 0:5, D = 0:1, Q = 0:5)

```{r}
fit_model <-
  train_data %>%
  model(
    sarima = ARIMA(
      log(withdrawals) ~ 0 + pdq(p = 0:5, d = 0:2, q = 0:5) + PDQ(P = 0:5, D = 0:1, Q = 0:5),
      stepwise = FALSE
    )
  )
fit_model %>% print()
```

```{r}
forecast::checkresiduals(augment(fit_model)%>%
                           rename(residuals=.resid) %>%
                           mutate(residuals= log(residuals) ) )
```
The residual analysis shows that our model is good and acf plot shows very few autocorrelation of residuals to its lags and its mostly white noise. the histogram of residuals are also pretty much normaly distrubuted.

## Forecast vs Actual
```{r}
## if we already set n in new data, h is not required in forecast as number of forecast days are provided in new_data with n=  option.
frcst_data<- 
fit_model %>% 
    forecast(new_data = future_xreg
             )
frcst_data |>
  autoplot(
    final_data %>%  filter(TransDate >= as.Date(data_split) - days(0))
    ) 

```
The plot above shows forecast in blue and black line shows the actual values. We notice that model is underforecasting at the end of november and third week of december and
last of december is overforecasted which is due to christmas and newyear period. Where the cash withdrawals become quite low bcz people spend time with family or travel abroad. 

Lets add another arima model with dummy variables to account for these issues and see how our model performs.

## Train models
```{r}
fit_model <-
  train_data %>%
  model(
    dummy_arima = ARIMA(
      log(withdrawals) ~ 
        (Months == 11 & Days > 23) + 
        (Months == 12 & Days > 14) +
        (Months == 12 & Days == 24) +
        (Months == 12 & Days == 25) +
        (Months == 12 & Days == 26) +
        (Months == 12 & Days == 27) +
        (Months == 12 & Days == 28) +
        (Months == 12 & Days == 29) +
        (Months == 12 & Days == 30) +
        (Months == 12 & Days == 31),
      stepwise = FALSE
    ),
    sarima = ARIMA(
      log(withdrawals) ~ 0 + pdq(p = 0:5, d = 0:2, q = 0:5) + PDQ(P = 0:5, D = 0:1, Q = 0:5),
      stepwise = FALSE
    )
  )
fit_model %>% print()
```
Above we can see the selected ARIMA process for two models.

```{r}
glance(fit_model)
```
We can see that the dummy_arima model has lower AIC and AICs in comparison of Two sarima model without dummy variables.

```{r}
augment(fit_model) |>
  features(.innov, ljung_box, dof = 0, lag = 2 )
```
ljun_box test shows high statistis 44.65 for dummy_arima means that our model is good.


## best_model

```{r}
best_model = "dummy_arima"
forecast::checkresiduals(augment(fit_model)%>%
                           filter(.model==best_model)%>%
                           rename(residuals=.resid) %>%
                           mutate(residuals= log(residuals) ) )
```
```{r}
augment(fit_model)%>%filter(.model==best_model)%>% rename(residuals=.resid) %>%as.data.frame() %>%  summarise(res_mean= mean(residuals))
```


The time plot of the residuals shows that the variation of the residuals stays much the same across the historical data. Therefore the residual variance can be treated as constant. This can also be seen on the histogram of the residuals. The histogram suggests that the residuals are normal Consequently, forecasts from this method will probably be quite good.

## Create data for forecast
```{r}
## if we already set n in new data, h is not required in forecast as number of forecast days are provided in new_data with n=  option.
frcst_data<- 
fit_model %>% 
    forecast(new_data = future_xreg
             )
frcst_data |>
  autoplot(
    final_data %>%  filter(TransDate >= as.Date(data_split) - days(0))
    ) 

```
Dummy forecast shown in Red is much better than sarima forecast in Green and it captures the last week of November and december much better compared to sarima model without dummies.

```{r}
frcst_data %>% 
  as_data_frame() %>% 
  select(TransDate,.model, .mean) %>% 
  left_join(
    final_data %>%filter(TransDate >= as.Date(data_split) - days(0)) %>% select(TransDate, withdrawals),
          by= "TransDate") %>% 
  mutate(diff= (withdrawals - .mean)^2 ) %>% 
  group_by(.model) %>% 
  summarise(rmse= sqrt( mean(diff) )) %>% 
  arrange(rmse)


```
The error rmse is also way lower for dummy variable. 



we can generate a new time series that are similar to the observed series. To achieve this, we utilize a type of bootstrapping known as the Box-Cox bootstrap.

The first step in the process is to transform the time series using the Box-Cox transformation. This transformation helps to stabilize the variance of the time series and make it more suitable for analysis. Following the transformation, we decompose the time series into its trend, seasonal, and remainder components using the STL (Seasonal and Trend decomposition using Loess) method.

Once the decomposition is complete, we focus on generating new variations of the remainder component. To do this, we cannot simply use the re-draw procedure described in previous literature due to the presence of autocorrelation in an STL remainder series. Instead, we adopt a “blocked bootstrap” approach, where contiguous sections of the time series are selected at random and joined together to create new variations of the remainder component.

The resulting bootstrapped remainder series are then added to the trend and seasonal components obtained from the STL decomposition. Finally, the reverse Box-Cox transformation is applied to the result, giving us new variations of the original time series.

In conclusion, our proposed method utilizes a combination of Box-Cox transformation, STL decomposition, and blocked bootstrapping to generate new time series that are similar to the observed series.

## create bootstraped seies 100 times
```{r}
set.seed(121)
sim_forecast_data<-
fit_model[best_model] %>% 
  generate(new_data = future_xreg, times = 100, bootstrap = TRUE , bootstrap_block_size = 30)

final_data %>%  
  filter(TransDate >= as.Date(data_split) - days(90)) %>% 
  ggplot(aes(x = TransDate)) +
  geom_line(aes(y = withdrawals)) +
  geom_line(data = sim_forecast_data, aes(y = .sim, colour = as.factor(.rep) )) +
  #coord_polar()+
  labs(title="Daily cash demand", y="£GBP" ) +
  guides(col = FALSE)

```
Now we can forecast for all 100 bootstrap series with our tarined model and in the end we can forecast using the average of all forecasts. the resulting forecast can be seen below.

## Aggregated sim forecast

```{r}
sim_forecast_data1<-
sim_forecast_data %>% as.data.frame() %>%  group_by(TransDate) %>% 
  summarise(.sim= mean(.sim)) %>% 
  tsibble(index = TransDate)

final_data %>%  
  filter(TransDate >= as.Date(data_split) - days(90)) %>% 
  ggplot(aes(x = TransDate)) +
  geom_line(aes(y = withdrawals)) +
  geom_line(aes(y = .sim, colour = "red" ),
    data = sim_forecast_data1) +
  #coord_polar()+
  labs(title="Daily cash demand", y="£GBP" ) +
  guides(col = FALSE)
```
```{r}
sim_forecast_data1%>% 
  as_data_frame() %>% 
  select(TransDate,.sim) %>% 
  left_join(
    final_data %>%filter(TransDate >= as.Date(data_split) - days(0)) %>% select(TransDate, withdrawals),
          by= "TransDate") %>% 
  mutate(diff= (withdrawals - .sim)^2 ) %>% 
  summarise(rmse= sqrt( mean(diff) )) %>% 
  arrange(rmse)



```

